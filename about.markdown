---
layout: page
title: About
permalink: /neurips2023/about/
---

The unprecedented availability of data, computation and algorithms have enabled a new AI revolution, as seen in Transformers and LLMs, diffusion models, etc, resulting in revolutionary applications such as ChatGPT, generative AI 
and AI for science. However, all of these applications have in common an 
always-growing scale, which makes training models more difficult. This can be a bottleneck for the advancement of science, both at industry scale and for smaller research teams that may not have access to the same training infrastructure. 

The **Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization** aims to give all 
researchers the tools necessary to train neural networks at scale. It will provide a dynamic and collaborative platform for researchers, practitioners
to delve into cutting-edge advancements in neural network training. Our proposal focuses on addressing challenges 
to enhance computational efficiency, scalability, and resource optimization.

By optimizing the training process, we can accelerate innovation, drive impactful applications in various domains and enable progress in applications such as AI for good and for science. Our workshop will cover topics such as optimization of computations (re-materialization,  tensorized layers), and  parallelization across devices (offloading, different types of parallelisms, pipelining).

Through 
panel sessions, poster presentations, lightning talks, and open Q&A sessions, we aim to address crucial questions
related to these advanced techniques. Challenges such as managing increased computational complexity, handling data dependencies, optimizing algorithmic design, and striking the right balance between efficiency and model expressiveness will be tackled.
Furthermore, this workshop will bring together diverse communities, including industry and academia, theoretical researchers and practical application developers, as well as experts from the domains of high-performance computing and artificial intelligence. By fostering collaboration and knowledge exchange among these communities, we can drive interdisciplinary advancements in neural network training and foster the formation of vibrant communities with shared interests.

