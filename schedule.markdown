---
layout: page
title: Schedule
permalink: /neurips2023/schedule
---
<style>
/* div {
    text-align: center;
    margin: 0 auto;
} */
</style>
ğŸ‘‰ [WANT poll](https://forms.gle/cJHmvtZvdbMuHzxh9) - Tell us your insights and thought about efficient training of neural networks! Your vote does matter! 

ğŸ“œ [WANT page at OpenReview](https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/WANT) - Accepted papers (Orals & Posters) are here!

Workshop on Advancing Neural Network Training (WANT) will take place on **December 16, 2023**
- **offline**: at the venue of the [NeurIPS 2023 conference](https://neurips.cc) in New Orleans, USA, **room 243-245**,
- **online**: with [streaming from the venue](https://neurips.cc/virtual/2023/workshop/66493) ğŸ¥, poster session and networking in Gather Town ğŸ°.

<!-- 
<table><tbody>
<tr>
  <th> Time (New Orleans) </th>
  <th> 
  
  **Morning** 
  
  </th>
<tr>
<tr>
  <td>
  08:15 - 08:45 
  </td>
  <td>
<details>
<summary> 

Invited talk <br> **Rematerialization algorithms for Memory-efficient learning** <br> *Lionel Eyraud-Dubois* 

</summary> 

**Abstract:** The training phase of Deep Neural Networks is often a very memory-intensive procedure, where large amounts of intermediate data have to be kept in memory during one iteration. One possible approach to reduce memory usage is rematerialization, aka gradient checkpointing, where some intermediate data are recomputed when needed rather than kept in memory. This provides a tradeoff between memory usage and recomputation time. In this talk I will present several approaches for the optimization problem, where one wants to minimize the recomputation time given a fixed memory budget. The corresponding algorithms have been implemented in easy-to-use libraries for the PyTorch framework, which can significantly reduce memory usage with reasonable overhead.
</details> 
  </td>
<tbody></table>
 -->



| Time (New Orleans) | **Morning**   | 
|:-----------------------------------------------------------------:|
| 08:15 - 08:45 | Poster placement                             | 
| 08:45 - 09:00 | Welcome speech from Organizers ğŸ¥                                  | 
| 09:00 - 09:30 | Invited talk ğŸ¥  <br> **A data-centric view on workflows that couple HPC with large-scale models** <br> *Ana Gainaru*                             | 
| 09:30 - 10:00 | Invited talk ğŸ¥  <br> **Rematerialization algorithms for Memory-efficient learning** <br> *Lionel Eyraud-Dubois*                      | 
| 10:00 - 10:30 | Coffee break  ğŸ°                                | 
| 10:30 - 11:00| Invited talk ğŸ¥ <br> **Navigating the Landscape of Enormous AI Model Training** <br> *Yang You*                        | 
| 11:00 - 11:30 | Invited talk ğŸ¥ <br> **Enabling efficient trillion parameter scale training for deep learning models** <br> *Tunji Ruwase*                           | 
| 11:30 - 12:00 | Contributed talks ğŸ¥ | 
| 11:31 - 11:36 | Contributed talk ğŸ¥ <br> **Training and inference of large language models using 8-bit floating point** <br> *Sergio Perez, Yan Zhang, James Briggs, Charles Blake, Josh Levy-Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, Andrew Fitzgibbon*   |
| 11:37 - 11:42 | Contributed talk ğŸ¥ <br> **MatFormer: Nested Transformer for Elastic Inference** <br> *Fnu Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain*    |
| 11:43 - 11:48 | Contributed talk ğŸ¥ <br>  **Sparse Backpropagation for MoE Training** <br> *Liyuan Liu, Jianfeng Gao Â· Weizhu Chen*  |
| 11:49 - 11:54 | Contributed talk ğŸ¥ <br> **Efficient Parallelization Layouts for Large-Scale Distributed Model Training** <br> *Johannes Hagemann, Samuel Weinbach, Konstantin Dobler, Maximilian Schall, Gerard de Melo*   |
| 11:55 - 12:00 | Contributed talk ğŸ¥ <br> **CoTFormer: More Tokens With Attention Make Up For Less Depth** <br> *Amirkeivan Mohtashami, Matteo Pagliardini, Martin Jaggi*  |

| Time (New Orleans) | **Afternoon**    |
|:-----------------------------------------------------------------:|
| 12:00 - 13:00 | Lunch  ğŸ° |
| 13:00 - 13:30 | Lunch (offline) \| Poster session (Gather Town) ğŸ° 
| 13:30 - 14:00 | Poster session (offline + Gather Town) ğŸ°                                | 
| 14:00 - 14:30 | Invited Talk ğŸ¥ <br> **Crafting Computational Efficiency for Large Models: Training Recipes, Scaling Strategies and Sparsity Sorcery with Specialized Hardware** <br> *Natalia Vassilieva*                           | 
| 14:30 - 15:00 | Invited Talk + Q&A (Naveen Rao) ğŸ¥                           | 
| 15:00 - 15:30 | Coffee break ğŸ°                                | 
| 15:30 - 16:00 | Invited Talk ğŸ¥ <br> **Efficient LLM Training and Inference on GPUs** <br> *Mohammad Shoeybi, Bryan Catanzaro*                           | 
| 16:00 - 16:50 | Panel Discussion ğŸ¥ <br>  *Yang You, Tunji Ruwase, Natalia Vassilieva, Mohammad Shoeybi, Jean Kossaifi* |
| 16:50 - 17:00 | Closing remarks ğŸ¥ |
| 17:00 - 17:30 | Poster session (offline + Gather Town) ğŸ°                                | 

<!-- | 16:00 - 16:50 | Panel Discussion <br> {::nomarkdown}<ul><li>Yang You </li> <li> Olatunji Ruwase </li>  <li> Natalia Vassilieva </li>  <li>Mohammad Shoeybi </li> <li>Jean Kossaifi</li></ul>{:/} | -->

<!-- | **Activity (morning)**    | **Duration** |
|-----------------------------------------------------------------|--------------|---------------------------------------------------------------|--------------|
| Welcome speech from organizers                                  | 10 mins      | 
| Invited Talks (3-4)                               | 15+5 mins (each)   | 
| Coffee break + Poster Session                                   | 30 mins      |
| Panel Discussion with invited speakers  | 40 mins      |
| Lightning session            | 40   mins    | 


| **Activity (afternoon)**    | **Duration** |
|-----------------------------------------------------------------|--------------|---------------------------------------------------------------|--------------|
| Lunch break + Poster Session                                  | 90 mins      |
| Contributed talk (Best Paper)                                 | 10 mins      |
| Invited talks (3-4)                            | 15+5 mins  (each)   |
| Coffee break + Poster Session                                 | 30 mins      |
| Panel Discussion with invited speakers  | 40 mins      | -->